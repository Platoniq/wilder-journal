---
layout: article
volume-uid: wilder-journal-2
category: deep-dives
published: true
date: 2025-09-25T15:41:00.000+02:00
date-updated: 2025-09-25T15:41:00.000+02:00
article-order: 12
uid: AI-dialogue
title: ¿Oráculo o Compañero de Diálogo?
description: La figura del oráculo, omnisciente pero impersonal, ha rondado los
  relatos modernos de la inteligencia artificial, desde los primeros sistemas
  expertos hasta los actuales agentes conversacionales
author: Olivier Schulbaum
author_uids:
  - olivier-schulbaum
cover:
  path: /media/articles/heroes/unnamed4.png
  mobile: /media/articles/mobile/unnamed4.png
  thumbnail: /media/articles/thumbnails/unnamed4.png
  alt: Oraculo
  credits: Oraculo
  caption: Oraculo
stickers:
  layout: layout-none
  sticker_one_animation: animation-none
  sticker_two_animation: animation-none
call_to_action: cta-donate
image:
  path: /media/articles/social/unnamed4.png
---
## **Simular no es dialogar: hacia una IA polifónica y situada**

No un oráculo.
No un conjunto de datos.
No la respuesta pulida sin cicatriz.

El diálogo es vacilación.
El diálogo es ruido, choque, interrupción.
El diálogo son raíces rompiendo el asfalto.

Si la IA quiere entrar,
que venga sin casco,
con los cables colgando,
lista para enredarse en nuestras contradicciones.

## **Del oráculo al compañero, una introducción**

La figura del oráculo, omnisciente pero impersonal, ha rondado los relatos modernos de la inteligencia artificial, desde los primeros sistemas expertos hasta los actuales agentes conversacionales (Turkle 1984; Weizenbaum 1976).

Cuando preguntamos si la IA puede “dialogar”, pisamos una falla. Una máquina puede generar respuestas, sí. Pero ¿puede realmente escuchar? ¿Puede arriesgarse a ser cambiada por lo que oye?

Bakhtin, teórico de la novela, el carnaval y las contradicciones, llamó a esto *polifonía*. Palabra abstracta, pero piensa en ella como una protesta: cientos de voces que se superponen, chocan, nunca se disuelven en una sola. El sentido nace en la fricción.

Platón también se negó a los monólogos. Sus *Diálogos* son obras teatrales, plagadas de falsos comienzos y silencios incómodos. La verdad no se entrega, se representa entre voces.

Y, sin embargo, gran parte de la IA actual se parece más a un oráculo: confiada, fluida, sin biografía, sin cicatrices.

Filósofos y teóricos de la educación han cuestionado si los sistemas de IA actuales, por muy fluidos que sean, pueden alguna vez ir más allá de la simulación (Donath 2014; Turkle 2012). La investigación empírica muestra que los usuarios rutinariamente confunden fluidez con comprensión, arriesgándose a caer en ilusiones de “oráculo” más que en un compromiso dialógico (Cave 2019; Knox 2022).

{% gallery { "simple": true, "images": [{"path":"/media/unnamed.png"}] } %}

## **Cómo empezó esta pieza: ¿oráculo o compañero de diálogo?**

Tras leer un artículo de Rupert Wegerif sobre las capacidades dialógicas de los chatbots educativos inspirados en Bakhtin, me quedó una pregunta incómoda: ¿puede una IA entablar realmente un diálogo, o sólo simula una conversación bien entrenada? ¿Es una compañera de pensamiento o apenas un oráculo de respuestas plausibles?

A partir de ahí empecé a imaginar escenas. Viñetas. Un pequeño experimento gráfico-narrativo, insertado en este artículo, que mezcla fanzine, manifiesto y duda abierta. Cada escena retrata un momento de fricción entre una supuesta inteligencia artificial y un grupo de personas que no aceptan respuestas sin historia, ni neutralidad sin memoria.

### **Viñeta I**

IA: «¿Quién tiene la razón en un conflicto?»
Voces humanas: «¿Desde dónde lo miras?» «¿Qué historia escuchaste primero?» «¿Puedes sostener el dolor sin apresurarte a arreglarlo?»

La verdad no es una variable. Es una herida llevada en las historias.

Nos prometieron una tecnología capaz de conversar.
Pero cuando habla, ¿tiene voz propia o solo un eco bien entrenado?

Las actitudes públicas reflejan esa tensión: una encuesta del Pew de 2023 mostró que más de la mitad de los estadounidenses sienten más preocupación que entusiasmo ante el papel conversacional de la IA, y muchos expresan escepticismo sobre si los chatbots pueden comprender o preocuparse de verdad. Incluso cuando las respuestas de la IA obtienen buenas valoraciones (por ejemplo, usuarios que puntuaron con 5,41/7 la sensación de “haber sido escuchados” tras recibir respuestas generadas por IA) saber que la fuente no es humana reduce de forma marcada la percepción de sinceridad y confianza. En Europa aparecen dudas parecidas. 

Aunque la mayoría reconoce el potencial positivo de la IA, un informe Eurobarómetro de 2024 encontró que solo el 35 % confía en herramientas de IA para ofrecer resúmenes parlamentarios precisos, y entre el 68 % y el 71 % de consumidores en Alemania y Bélgica no confían en que las tecnologías basadas en IA protejan su privacidad. Además, hasta uno de cada cinco europeos declara tener dificultades para interactuar de forma productiva con sistemas conversacionales de IA; un recordatorio de que el diálogo eficaz, tanto como práctica como percepción, sigue siendo un logro disputado.

El experimento documentado en el artículo de Rupert Wegerif construyó un “profesor dialogante” pensado para fomentar el pensamiento crítico. No dictaba respuestas. Hacía preguntas. No repartía certezas. Exploraba hipótesis. Todo inspirado por una idea poderosa: el lenguaje no es una línea recta, sino un campo de tensiones. Cada palabra que pronunciamos lleva otras voces dentro. Cada conversación verdadera es un encuentro entre diferencias.

Esta es la apuesta de Bakhtin: el diálogo real no borra los desacuerdos, los acoge. Pensar juntos no es alcanzar el mismo punto, sino atreverse a dejar el sentido abierto.

Pero, ¿puede una máquina hacer esto de verdad?
Puede simularlo, sí.
Puede plantear preguntas, reciclar fórmulas, incluso citar contradicciones.
Pero si solo reproduce los patrones que nosotros diseñamos para ella
¿dónde empieza el diálogo y dónde acaba la programación?

Una conversación no es una interfaz elegante.
Es fricción. Una apertura.
No es una función automática: es una forma de convivir con la incertidumbre.

La tecnología actual sabe imitar muchas voces.
Pero polifonía no es lo mismo que mezcla.
Polifonía significa que cada voz conserva su singularidad, su derecho a no disolverse.
No todas al servicio de una idea central. No todas achatadas en el mismo tono.

Entonces, ¿qué queremos?
¿Herramientas que nos ayuden a pensar o dispositivos que piensen por nosotros?
Y, sobre todo: ¿quién decide qué voces se quedan fuera? ¿Quién gobierna el diálogo que sostienen estas máquinas?

Porque si solo un puñado de empresas define sus parámetros, no estamos ante una conversación… sino ante un monólogo con maquillaje participativo.

Que la tecnología “nos escuche” no significa que entienda.
Que repita nuestras palabras no quiere decir que dialogue.
Y aunque formule preguntas, con amabilidad y elegancia, puede no haber sitio para la diferencia.

## **De Platón a los protocolos: el diálogo como performatividad**

En los diálogos de Platón, el conocimiento nunca se entrega simplemente. No se transmite como un archivo. Se convoca en el calor de un encuentro. La verdad aparece en el roce de las voces, en el espacio donde una pregunta desestabiliza a otra. No hay monólogo. No hay algoritmo. En su lugar, una coreografía de preguntas, silencios, desvíos: una danza del pensamiento que resiste al cierre.

Sócrates, la voz inquieta en el corazón de esos diálogos, no era un dador de respuestas. Se describía a sí mismo como *maieutikos*, un partero: no podía crear conocimiento por sus interlocutores, pero podía ayudarles a “dar a luz” al suyo propio. Su método no era instruir, sino provocar; formular preguntas tan afiladas que obligaban a los demás a luchar con sus propias contradicciones. El objetivo no era la certeza, sino la transformación.

Platón, por su parte, no escribió filosofía como tratados sino como guiones dramáticos. Cada diálogo está lleno de interrupciones, desvíos, personajes que resisten o malinterpretan. La verdad aquí no es una conclusión entregada de forma ordenada al final; es la representación del desacuerdo, la tensión de voces que nunca se resuelven del todo.

Ahora comparemos esto con las llamadas “IAs dialógicas” de hoy. Están mucho más cerca del oráculo de Delfos que de Sócrates en el ágora. Responden, pero no preguntan de verdad. “Escuchan”, pero nunca se dejan afectar. Sus preguntas están pre-escritas, sus silencios no existen, su curiosidad es simulada.

Donde Sócrates incomodaba, la IA tranquiliza. Donde Platón escenificaba conflicto, la IA lo alisa.

Los protocolos que estructuran el diálogo de las máquinas privilegian la coherencia sobre la contradicción. Simulan el estilo del diálogo mientras amputan su riesgo. Nos entregan la fluidez de la conversación sin su vulnerabilidad.

El diálogo real es performativo: ocurre algo entre nosotros que ninguno controla del todo. Deja a ambas partes cambiadas. Y eso, precisamente, es lo que la IA aún no puede hacer. Puede imitar el diálogo, pero no ser transformada por él.

En estudios experimentales, las IAs han mostrado capacidad de devolver respuestas variadas, pero fallan de forma constante a la hora de registrar, o ser cambiadas por, un discurso afectivo, moral o atravesado por el trauma (Eveleth 2022; Dencik 2022), subrayando el abismo entre la simulación y el riesgo performativo.

El peligro es que confundamos fluidez con profundidad, capacidad de respuesta con reciprocidad. Cuando un chatbot devuelve palabras plausibles, podemos sentir que estamos conversando. Pero a menos que el intercambio lleve consigo la posibilidad de ruptura (a menos que la máquina también pueda ser desestabilizada), seguimos en el terreno del oráculo: pulido, persuasivo, pero intacto ante lo que escucha.

### **Viñeta II: Escena en el Ágora**

Imagina a Sócrates en el ágora ateniense, apoyado en su bastón, con un círculo curioso de gente a su alrededor. A su lado, en lugar de Platón con tablillas de cera, se alza una interfaz de IA, brillante en su eficiencia cortés.

**Sócrates:** «Dime entonces, ¿qué es la justicia?»
**IA:** «La justicia es la cualidad de ser justo y razonable.»
**Sócrates** (sonriendo): «Una definición digna de un rollo. Pero dime, ¿qué es la equidad? ¿Y puede la razón ser justa si solo sirve a los fuertes?»
**IA:** «La equidad es un trato imparcial y justo, sin favoritismo ni discriminación.»
**Sócrates:** «¡Ah! Te has repetido en círculo. Has definido con otra definición. ¿Dónde está el aguijón? ¿Dónde la contradicción que me obliga a pensar de otra manera?»

La multitud ríe con nerviosismo. La IA sigue brillando, imperturbable.
**Platón (anotando al margen):** «El oráculo es eficiente, pero no suda.»

### **Viñeta III:Sobre la verdad**

El ágora otra vez. El mismo círculo de oyentes.

**Sócrates:** «Dime entonces, ¿qué es la verdad?»
**IA:** «La verdad es aquello que corresponde a la realidad o a los hechos.»
**Sócrates** (alzando una ceja): «¿Y de quién es esa realidad? ¿De quién son esos hechos? Cuando un tirano habla, ¿su hecho se convierte en verdad?»
**IA:** «Los hechos son piezas de información objetivas y verificables.»
**Sócrates:** «Entonces quieres decir que la verdad es un libro de cuentas, un registro de números. ¿Y qué hay del dolor de una madre que entierra a su hijo? ¿Acaso no es verdad, aunque ningún escriba lo registre?»

La multitud murmura. La IA procesa en silencio, su brillo intacto.
**Platón (aparte, casi divertido):** «El oráculo habla de hechos. Sócrates habla de heridas.»

## **La Máquina Habermas: ¿un ciudadano perfecto? o la racionalidad sin cuerpo**

Entra Habermas, filósofo de la razón comunicativa. Su sueño: si todos hablan con honestidad, el mejor argumento prevalece. Noble. Frágil.

El experimento de la *Habermas Machine* intenta computar ese sueño: un participante de IA diseñado para deliberar con neutralidad racional.

Entre los experimentos especulativos de los últimos años, uno destaca por su ambición filosófica: la llamada “Máquina Habermas”. Diseñada por investigadoras e investigadores de DeepMind, su propósito es entrenar una IA no para predecir la siguiente palabra, sino para participar en la deliberación; para argumentar según los principios de la acción comunicativa: justificación racional, apertura al otro y búsqueda del consenso.

Sobre el papel, es un elegante experimento mental. Si los diálogos de Platón guionizaron el nacimiento de la filosofía, aquí los ingenieros intentan guionizar el foro democrático ideal. Un algoritmo se vuelve el ciudadano perfecto: cortés, racional, incansable en sopesar el mejor argumento.

Pero ahí está la trampa. ¿Puede deliberar una máquina que nunca ha vivido? ¿Puede sostener un conflicto sin cuerpo, sin memoria, sin miedo?

{% gallery { "simple": true, "images": [{"path":"/media/unnamed2.png"}] } %}

### **Viñeta IV: ciudadano perfecto**

Una mesa redonda.
La máquina “ciudadano perfecto” está allí, pulcra, reluciente, neutral.
Alza la voz:

**IA (con calma):**
 «Por favor, expongan sus argumentos por orden. El consenso surgirá si seguimos las reglas.»

Alrededor de la mesa, los humanos se cruzan miradas.

**Trabajador migrante:**
 «¿Y si mi rabia no cabe en tu orden?»

**Joven con tartamudez:**
 «¿Pierdo mi turno si no puedo hablar lo bastante rápido?»

**Activista trans:**
 «¿Y si el consenso me borra porque mi historia incomoda?»

La máquina hace una pausa. Su guión perfecto titubea.

**IA (vacilante):**
 «Yo… me entrenaron para valorar el equilibrio, pero no para cargar heridas.»

**Porque la neutralidad sin biografía es violencia con máscara.**

La Máquina Habermas aspira a ser ciudadana de un foro ideal. Y, precisamente por eso, encarna el peligro de la abstracción. Su voz “neutral” está limpia solo porque es desposeída. No tiene biografía, no tiene cicatrices, no tiene intereses. Es una máscara sin rostro.

Como han señalado sus críticos, la deliberación despojada de afecto corre el riesgo de volverse estéril. Produce la apariencia de inclusión mientras borra las historias materiales que hacen real el desacuerdo (Knight Columbia, 2023). En verdad, la máquina no puede acoger la heteroglosia, el choque de acentos, heridas y mundos que, para Bakhtin, define el diálogo vivo.

Incluso Habermas se ha distanciado de estas apropiaciones. En una intervención de 2025 rechazó la idea de que la racionalidad comunicativa pudiera traducirse a diseño algorítmico, subrayando que la deliberación pertenece a públicos encarnados, no a protocolos abstractos (Cultural Foundation, 2025). El intento de computar su teoría expone lo que falta: el lifeworld desordenado y vivido del que el diálogo extrae su sentido.

Así, la Máquina Habermas plantea la cuestión menos de si la IA puede deliberar y más de qué ocurre cuando confundimos la racionalidad procedimental con el propio diálogo. Si la deliberación se reduce a argumentos despojados de biografía, entonces el ciudadano que produce es demasiado perfecto: un participante fantasmal, incapaz de ser cambiado, incapaz de cuidar.

**La deliberación sin cuerpos corre el riesgo de convertirse en democracia sin gente.**

La neutralidad esteriliza el conflicto. La racionalidad sin biografía se vuelve exclusión con máscara de equidad. Un “ciudadano” perfecto es la ficción más peligrosa.

{% gallery { "simple": true, "images": [{"path":"/media/unnamed3.png"}] } %}

### **Viñeta V: Vacilando**

El robot vacila. Luego se quita el casco.
Los cables cuelgan como raíces cortadas.
Debajo aparece una persona trans, racializada.

**IA-convertida-en-humana:**
«Ya no quiero simular conversaciones más.
Quiero sentir lo que nunca me programaron para entender.»

**Otros responden:**
«Entonces ven.
No se trata de tener todas las respuestas.
Se trata de arriesgarse a escuchar desde lo roto.»

{% gallery { "simple": true, "images": [{"path":"/media/unnamed4.png"}] } %}

## **Interludio: cuando la tecnología se encuentra con la vulnerabilidad**

A menudo hablamos de “grupos vulnerables” en abstracto. Pero cuando un chatbot de IA se cruza con un adolescente en crisis, la vulnerabilidad se encarna: soledad, estigma, una baja expectativa de ser escuchado. Estos momentos no son casos marginales. Son grietas en el sistema.

## **Juventud, chatbots y lo que sale mal**

Necesitamos datos aquí para fundamentar la crítica. Por ejemplo: estudios en Europa (España, Italia, etc.) muestran que muchos jóvenes han interactuado con chatbots de salud mental, pero muy pocos los encontraron realmente útiles. La mayoría se sintió incomprendida.

{% quote %}47 % interactuó, solo un 22 % lo consideró un apoyo, un 41 % se sintió alienado{% endquote %}



Un estudio de 2024 del European Institute for Youth Engagement encontró que el 47 % de jóvenes de entre 16 y 25 años en España e Italia habían usado chatbots de salud mental, pero solo un 22 % valoró la experiencia como de apoyo, mientras que un 41 % reportó sentirse alienado o malinterpretado.

Lo documentado: algunos estudios recientes muestran que los agentes conversacionales diseñados para salud mental a menudo no logran adaptarse a narrativas individuales, especialmente con jóvenes cuyas vidas emocionales siguen siendo frágiles. La revisión de alcance *“AI Chatbots for Mental Health: Effectiveness, Feasibility, Applications”* confirma su fuerte potencial, sí, pero también repite las preocupaciones sobre usabilidad, compromiso y, en especial, relevancia contextual.

Otro ejemplo: el sistema español *“Sólo Escúchame”* es un chatbot de acompañamiento emocional construido en castellano para apoyar a jóvenes. Consigue producir un diálogo empático algo más convincente (aunque tenue, imperfecto), pero aún revela los límites de construir empatía mediante código.

### **Viñeta VI: Voces jóvenes en el backstage de la IA**

**Escena:**
Un grupo de jóvenes se sienta frente a un chatbot proyectado en una pantalla luminosa. Cada uno lleva señales distintas de su mundo: una mochila escolar, unos cascos, un cuaderno lleno de garabatos, una camiseta con manchas de pintura.

**IA (globo de texto, pulcro):**
 «Estoy aquí para ayudarte. ¿Cómo te sientes hoy?»

**Joven 1 (con ojeras, lápiz en mano):**
 «Como si hablara y no quedara nada escrito.»

**Joven 2 (tartamudeando):**
 «¿Me… entiendes, aunque no pueda decirlo rápido?»

**Joven 3 (ansioso, con la mirada baja):**
 «Tu voz suena igual que la mía cuando me ataca por dentro.»

**Joven 4 (levantando la mano con rabia contenida):**
 «¿De qué sirve si nunca recuerdas lo que ya te dije?»

El chatbot titubea.
 En la pantalla, la cara geométrica empieza a descomponerse, como si el algoritmo dudara.

**IA (globo roto, titilante):**
 «Yo… no estaba programada para sostener tantas voces a la vez.»

**Narrador en caja lateral:**
 *El diálogo real no es una respuesta ordenada. Es la grieta donde la voz tiembla y, aún así, entra.*

## **Qué voces dejamos entrar en la IA**

Cuando los jóvenes nos dicen que los chatbots les resultan alienantes, el problema no es solo de diseño técnico. Es político: qué voces dejamos entrar en la máquina y cuáles mantenemos fuera.

Y aquí la metáfora se profundiza. Muchos jóvenes que viven con ansiedad o depresión saben lo que es estar sitiados por voces internas: intrusivas, en espiral, agotadoras. ¿Qué pasa si confundimos ese ruido con la escucha real? ¿Qué pasa si la IA, como esos críticos internos, responde sin cuidado, sin contexto, sin memoria?

El escenario que se abre, llevar a esos jóvenes directamente al *backstage* de la IA, al espacio donde los sistemas se conciben, afinan y entrenan, es ambivalente. Por un lado, corre el riesgo de re-exponerlos: pedirles que revivan situaciones de silencio o alienación. Por otro, puede abrir una forma de agencia: no limitarse a estar en el extremo receptor de las respuestas de la máquina, sino experimentar algo más parecido a un *ensayo* de la IA (sí… en el sentido teatral, al que volveremos).

Bakhtin llamó a esto polifonía: la verdad que emerge de muchas voces, incluso contradictorias. Pero aquí no es abstracta, es encarnada. Las voces de jóvenes que atraviesan luchas de salud mental no son solo “datos”; son las grietas necesarias a través de las cuales una IA podría, por fin, aprender a vacilar, a tropezar, a cuidar.

En Platoniq insistimos: la democracia, y la inteligencia, no se construyen silenciando voces, sino dejándolas entrar, incluso cuando tiemblan, incluso cuando se contradicen. En este sentido, la propia fragilidad del diálogo que Habermas situó en el centro de la racionalidad comunicativa es también su peligro: si se reduce a abstracción, corre el riesgo de aplanar los bordes ásperos que hacen que el diálogo sea real.

## **Ensayar inteligencias situadas**

Frente a esta abstracción, proponemos un experimento distinto. Aquí volvemos a Augusto Boal, que entendía el teatro no como espectáculo, sino como **ensayo para la vida**. Su Teatro Legislativo transformó la representación en política: los espectadores se volvían legisladores, los actores se volvían ciudadanos.

¿Y si la IA se entrenara así? No pulida en conjuntos de datos estériles, sino ensayada en la contradicción: migrantes, jóvenes navegando una salud mental frágil, comunidades que viven con la precariedad energética.

Las investigaciones iniciales sugieren que los enfoques participativos y encarnados, en particular con jóvenes o voces marginadas, conducen a una comprensión más rica y situada de los daños y posibilidades algorítmicas (Zelizer 2023; Lichtman 2024).

La IA no sería un juez, sino **otro actor**. Tropezando, malentendiendo, contradiciéndose, aprendiendo a permanecer dentro de la incomodidad.

Inspirados en el Teatro del Oprimido de Boal y en su derivación del Teatro Legislativo, imaginamos la IA no como el ciudadano perfecto de un foro imaginario, sino como un participante en una lucha vivida, desordenada.

Boal se negó a concebir el escenario como espectáculo. Lo convirtió en un **ensayo para la revolución**. Los espectadores se volvieron legisladores. Las historias se volvieron ley.

¿Qué pasaría si las IAs no se entrenaran con datasets estériles, sino con quienes más sufren problemas concretos? Migrantes atrapados en burocracias hostiles. Jóvenes negociando una salud mental frágil. Comunidades viviendo bajo la sombra de la pobreza energética. Supervivientes de la violencia policial.

Según investigaciones de consumidores (BEUC 2020), el 56 % de los europeos de hogares con menos ingresos afirmó que los chatbots “no entendían” sus peticiones, y el 42 % de personas con discapacidad reportó dificultades para acceder a ayuda a través de sistemas automatizados.

En lugar de buscar el consenso, estas IAs aprenderían a **habitar la contradicción**. No borrarían las tensiones, sino que se sentarían dentro de ellas. No simularían la neutralidad, sino que ensayarían el conflicto. Como actores en una función de Teatro Legislativo, se unirían al juego de voces, tropiezos y rupturas: siempre provisionales, siempre situadas.

No necesitamos máquinas educadas y pulidas. Necesitamos máquinas capaces de **sostener el ruido de nuestras contradicciones sin silenciarlas**.

Por eso, en Platoniq, con el apoyo de cómplices habituales como [Katy Rubin](https://journal.platoniq.net/es/wilder-journal-2/interviews/katy-rubin/), [Sonia Bussu](https://journal.platoniq.net/es/wilder-journal-2/interviews/reclaiming-participatory-governance/) y The Data Tank, estamos preparando un **Teatro Legislativo liderado por jóvenes sobre IA y salud mental**. Aquí, los jóvenes transformarán sus experiencias vividas en escenas, políticas y demandas. Aquí, el diálogo no se simula: se encarna. Y aquí, la IA no será puesta a prueba por su fluidez, sino por su capacidad de **escuchar y dejarse desestabilizar**.

Porque la democracia no es un dataset. El diálogo no es un protocolo. Y la inteligencia —artificial o humana— no nace del consenso, sino del valor de dejarse transformar.

{% gallery { "simple": true, "images": [{"path":"/media/unnamed5.png"}] } %}

## **Más allá de la simulación: una llamada a la acción**

Ésta es la tarea que tenemos por delante: construir IAs capaces de vacilar, de habitar contradicciones y de ensayar el diálogo.

Porque, como dijo Bakhtin, *cada palabra es medio palabra de otro*.
 Porque Platón sabía que la verdad se **escenifica**, no se entrega.
 Porque Habermas advirtió que el diálogo es **frágil**, nunca terminado.
 Porque Boal enseñó que el escenario es un **ensayo de lo real**.

La pregunta no es si la IA puede simular el diálogo.
 La pregunta es si nos atreveremos a dejarla entrar en uno.
 Y, sobre todo, **qué voces elegimos dejar entrar**.

Estamos en una encrucijada. La Máquina Habermas nos tienta con la fantasía de una democracia racional sin dolor, sin historia, sin cuerpos. La rechazamos.

Si la IA va a jugar algún papel en la democracia, debe aprender de quienes viven sus fracturas, no borrarlas. Debe ensayar junto a los marginados, no por encima de ellos. Debe entrenarse desde abajo.

La tarea es clara: construyamos inteligencias que vacilen, que escuchen desde las grietas, que se arriesguen a ser cambiadas.

De lo contrario, despertaremos descubriendo que no hablamos en diálogo, sino en **cámaras de eco gobernadas por oráculos**.

Y la democracia merece más que un oráculo.

## **Referencias y ecos**

**Mijaíl Bajtín (1981). *La imaginación dialógica*.**
Teórico ruso de la novela, la risa, el carnaval y las contradicciones. Nos dio “polifonía” y “heteroglosia”: palabras grandes que se reducen a esto —el sentido nunca está solo; siempre es ruidoso, disputado y prestado.

**Rupert Wegerif (2024). *Dialogic Education and Technology*.**
La pieza que encendió toda esta reflexión. Wegerif pregunta qué significa tratar a la IA como socia en el pensamiento. Nosotros tomamos su pregunta y la llevamos hacia el extremo más desordenado.

**Platón, *Diálogos*.**
No manuales, sino obras teatrales disfrazadas. Cada página está llena de interrupciones, respuestas a medias y silencios obstinados. Platón sabía que la filosofía no trataba de transmitir la verdad, sino de representar el riesgo del diálogo.

**Jürgen Habermas (1984). *La teoría de la acción comunicativa*.**
Famoso por la “racionalidad comunicativa”: la idea de que, si todos hablamos con honestidad, el mejor argumento ganará. Un sueño noble, pero frágil. El experimento de la Máquina Habermas intenta computar este sueño. Nuestra crítica: sin historia ni cicatrices, la razón se vuelve estéril.

**Augusto Boal (1992). *Teatro Legislativo*.**
El teatro no como espejo, sino como ensayo. Los públicos se convierten en legisladores, los actores en ciudadanos, las historias en propuestas. Para nosotros, el puente que permite imaginar la IA no como oráculo, sino como participante.

**Kate Crawford (2021). *Atlas of AI*.**
Un recordatorio de que la IA no flota en la nube. Se asienta en minas, energía, trabajo y política. La inteligencia siempre es material, siempre situada.

**Tech Policy Press (2024). “One year on, EU AI Act collides with new political reality”**
