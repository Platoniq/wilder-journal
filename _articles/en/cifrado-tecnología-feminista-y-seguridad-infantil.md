---
layout: article
volume-uid: wilder-journal-2
category: interviews
published: true
date: 2025-12-23T11:02:00.000+01:00
date-updated: 2025-12-23T11:02:00.000+01:00
article-order: 14
uid: Cifrado, tecnología feminista y seguridad infantil
interviewee: Hera Hussein
cover:
  path: /media/articles/heroes/captura-de-pantalla-2025-12-23-a-las-11.05.30.png
  mobile: /media/articles/mobile/captura-de-pantalla-2025-12-23-a-las-11.05.30.png
  thumbnail: /media/articles/thumbnails/captura-de-pantalla-2025-12-23-a-las-11.05.30.png
stickers:
  layout: layout-none
  sticker_one_animation: animation-none
  sticker_two_animation: animation-none
call_to_action: cta-donate
image:
  path: /media/articles/social/captura-de-pantalla-2025-12-23-a-las-11.05.30.png
---
At Platoniq, we often ask ourselves what it takes to create an online space where people can breathe—a space that allows disagreement without causing harm, that listens without recording, that protects without enclosing. Hera Hussein’s work with Chayn feels like an answer to that question: an act of feminist engineering in which care becomes a form of code.

### **Your work with Chayn has transformed the way technology can support survivors of gender-based violence. How did your personal journey lead you to found Chayn and to approach technology through a feminist lens?**

I started Chayn when I was 23, and I’m now 36, so in many ways I feel like I’ve grown alongside the organization. Chayn emerged from a deeply personal experience. I was helping two very close friends leave abusive relationships. Both were from Pakistan. One was living in the UK, where I was studying at university, and the other was in Pakistan, where I grew up.

What struck me was how difficult it was for them to find support. They struggled to understand what was happening to them, to recognize that it wasn’t their fault, to make sense of how the violence was affecting their mental health, and to find organizations that could help with legal issues, housing, or psychological support. Much of that information simply wasn’t accessible—especially online.

I decided to create Chayn because I couldn’t find that information myself, and because I was convinced that in a digital age there should be online spaces where people can find help. Especially for those who don’t feel able to make a phone call or physically go somewhere. There needed to be a space where they could do this privately and on their own terms. That was the origin of Chayn.

{% gallery { "simple": true, "images": [{"path":"/media/1_o7wxsxo6rrwuhoflx5qjqq.webp"}] } %}

### **In the INSPIRE project, we work on creating safe (or safer) digital spaces for democratic participation—understanding that “safety” is always a process, never a fixed condition. These are spaces that are inclusive, intersectional, and transparent, but also emotionally and psychologically safe. Based on your experience, which dimensions of digital participation spaces do you think are most often overlooked?**

Approaching technology from a feminist perspective came very naturally to me. It wasn’t a conscious decision; it was simply how I understood the world. I grew up in a very empowering environment, and I grew up surrounded by technology. It never occurred to me that there were things I couldn’t do because I was a woman.

However, when I began working professionally in the tech sector, gender inequality became impossible to ignore. It was—and still is—a deeply masculinized environment. Technology is designed according to parameters like speed, cost, or being “cool,” but “cool” is defined through a very narrow lens: what appeals to men, usually white European or American men, even when the products are intended for a global audience.

That led me to a fundamental question: what would technology look like if different people had a say in how it is designed? At Chayn, we produce technology starting from that question. We like technology, we build it, we use it, and we firmly believe that all of this can be done according to feminist principles.

### **You often speak about trauma-informed design and hope-based frameworks. How do these principles influence the way Chayn understands safety?**

Over the years, we’ve developed a set of trauma-informed design principles that guide all of our work. These principles help us think carefully about how we design policies, technologies, and content for people who may have lived through extreme trauma. One of these principles is hope.

A recent example is the artificial intelligence tool we launched to help survivors of image-based abuse submit takedown requests to platforms where images have been shared without consent.

This tool was co-created with 60 survivors from 20 different countries. We already had an initial idea based on years of working with survivors, but the co-design process completely transformed the final result. Participants raised key questions around privacy, usability, language, emotional safety, and the importance of clearly explaining what the tool can and cannot do.

Agency is one of our core principles. For us, that means ensuring that people using our services have real choices. Another key principle is accountability—not just acknowledging mistakes, but being transparent about limits: clearly explaining what a tool is for, what it isn’t for, and where its boundaries lie.

Hope runs through this entire process. Designing an AI tool with feminist principles alongside survivors is, in itself, an exercise in hope. Many people told us that participating made them feel that technology isn’t just something that happens to them, but something they can actively create and transform.

The tool was launched recently and has already been used by more than a hundred people. Because privacy is one of our central principles, we don’t collect detailed usage data. We know how many people use the tool, and we rely on voluntary feedback. Periodically, we invite users to share their experiences, and over time we’ll gain a clearer picture of its impact.

One of the first pieces of feedback we’ve received is that the platform helps survivors gather their story in one place, so they don’t have to repeat it over and over again. It also helps them understand how what they’ve experienced violates platform policies, thanks to clear and accessible summaries.

{% gallery { "simple": false, "images": [{"path":"/media/1_whxfenj-pkgjknballeuhq.webp"}] } %}

### **Speaking of data collection, the debate around encryption and child safety is often framed as a trade-off between privacy and protection. Why do you find this framing problematic?**

Because children deserve privacy too. They have digital rights of their own, and they also grow up to become adults who will continue to need privacy.

Of course, children need protection and safeguarding, as they can’t always make fully informed decisions. But it’s very easy for that concern to turn into surveillance. We should start from a foundation that respects the safety and privacy of everyone, including children.

There is no real contradiction between protecting children and preserving their private information. Framing it as a trade-off creates a false narrative that makes us feel we have to choose one over the other.

A child today may become a teenager tomorrow who begins protesting against a government. When they were younger, that government may not have had a problem with climate justice protests. But political contexts change, and suddenly private messages with friends or activist groups can be used against them.

Children have privacy rights not only as minors, but as future adults. That’s why this issue matters so much.

### **So can safety itself be radical? Can encryption—so often described as secrecy or isolation—become an instrument of collective care, even of democracy?**

Encryption provides foundational privacy. It allows information to remain confidential not only from abusive partners, but also from governments and other malicious actors.

Survivors, women, and girls are full citizens. And full citizens need private and confidential spaces. This doesn’t mean they don’t need protection, but much of that protection must happen outside encrypted spaces. That’s where our focus should be.

### **Can you share real-world examples where encrypted or privacy-centered tools have supported the safety and autonomy of people facing violence or exploitation?**

There are many examples. Encrypted messaging platforms are used by abortion providers in countries such as El Salvador, the United States, and Mexico. Feminists in countries like China, Iran, and Russia use VPNs and encrypted tools to organize around bodily rights, domestic violence, and women’s rights.

At a more basic level, organizations that support women and children need to protect extremely sensitive data: healthcare information, identity documents, personal histories. Encrypted services help prevent leaks and attacks.

We also know what happens when companies fail to protect data properly. A social app used by millions of women to talk about dating experiences was hacked, and all of that private information was made public because it wasn’t adequately secured.

Period-tracking apps are another example. Reproductive data is highly sensitive healthcare information. Platforms like Berlin-based Drip don’t store user data, which is crucial for people living in contexts where sexual activity, pregnancy, or reproductive decisions can put them at risk.

These are some of the reasons why women and girls not only should use encrypted platforms, but already do.

{% gallery { "simple": false, "images": [{"path":"/media/1_vuamnqzftj-kgistxc2yja.webp"}] } %}

### **Many survivors describe safety not only as “protection from harm,” but as the ability to speak, withdraw, or set the conditions of dialogue. How has Chayn transformed the way we design participatory or deliberative digital spaces, where power asymmetries and emotional exposure are inherent?**

I see Chayn as an exercise in hope, solidarity, and feminist reckoning. Women have the right to rigorous, practical information about their rights, their mental health, and the issues that shape their lives—presented in non-patriarchal ways that genuinely help keep them safe.

Women should be able to use the internet not only to consume content, but to learn, work, express themselves, build businesses, fall in love, leave relationships, raise children, hold opinions, and exist publicly without harassment.

These are freedoms men often take for granted. Women of all backgrounds—regardless of race, sexuality, disability, profession, or context—deserve digital spaces free from violence where those freedoms are possible.

At Platoniq, we understand (Safer) Spaces as placing emphasis on institutional responsibility, ensuring that organizations hosting digital participation take care of the emotional and ethical implications of the space, not just its technical configuration. From your experience, how can institutions adopt survivor-centered, trauma-aware governance practices when moderating or facilitating online dialogue?

Governments undoubtedly have a responsibility to protect their citizens. But that protection must be exercised in the right spaces. Today, there are multiple serious failures in the public realm that require urgent attention.

Survivors often wait five or six years for cases with clear evidence to be investigated. Many police forces lack training to address digital abuse. Complaints from sex workers are often not taken seriously at all. These are structural problems.

Preventing violence requires resources, cultural change, and real investment across all areas of social life. Surveillance of private communications is not a shortcut to safety.

The idea that governments will protect women and children simply by accessing everyone’s private messages is deeply flawed. That approach does not work, and it will not work.

### **What do you believe is the political impact of this approach to protection in our democratic world?**

From a law enforcement or governmental perspective, having access to large volumes of private information without judicial processes can seem very efficient.

But this is where the state oversteps its boundaries. The state should be a safety net, not a parental figure. And even families shouldn’t be constantly surveilling their children.

States are not always benevolent actors. There are systemic injustices, discrimination, and corruption. Power is asymmetrical. Governments already hold vast amounts of personal data, and we know that it is often mishandled.

Expanding surveillance is a political decision, not an inevitable necessity. It often responds more to convenience than to real protection, and safeguards are rarely sufficient to justify it.

{% gallery { "simple": false, "images": [{"path":"/media/1_17zt84sxshgsoypx2p0fhw.webp"}] } %}

### **You have long advocated for free software and transparency. How do you reconcile openness with privacy and protection?**

I don’t see any contradiction. Research funded with public money should be publicly accessible. Public spending should be transparent.

At the same time, private conversations, family life, and health data belong in the private realm. Openness should apply to power and institutions, not to people’s intimate lives.

### **What are the challenges of applying open-source principles in sensitive areas like gender-based violence or child protection?**

It requires careful management. At Chayn, we minimize data collection. Most of our resources are open and don’t require registration.

When people do create accounts—for example, to access human support via chat—we clearly explain how data is handled. We retain records for six months and then delete them. We don’t want to store information that could be requested by courts or misused.

All of our resources are released under open licenses, allowing organizations around the world to reuse and adapt them. We contribute to shared knowledge without compromising personal privacy.

### **To close, how can movements for digital rights and gender justice learn from one another?**

There is enormous potential for mutual learning, but too few shared spaces. Digital rights frameworks help feminist organizations understand how power, money, and technological systems interact. Gender justice movements help the digital rights field understand lived harms, contexts, and the complexity of real experiences.

Together, they can push for safer design, prevent harm, and build digital ecosystems that don’t merely react to violence, but actively create space for good to happen.

{% gallery { "simple": true, "images": [{"path":"/media/captura-de-pantalla-2025-12-23-a-las-12.22.42.png"}] } %}

Check the [DIY Online Safety Guide (2025)](https://www.chayn.co/guides/diy-online-safety)
