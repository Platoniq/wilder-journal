---
layout: article
volume-uid: wilder-journal-2
category: deep-dives
published: true
date: 2025-09-25T15:41:00.000+02:00
date-updated: 2025-09-25T15:41:00.000+02:00
article-order: 5
uid: AI-dialogue
title: Oracle or Dialogue Partner?
description: "The figure of the oracle, all-knowing but impersonal, has haunted
  modern accounts of artificial intelligence, from early expert systems to
  current conversational agents "
author: Olivier Schulbaum
author_uids:
  - olivier-schulbaum
cover:
  path: /media/articles/heroes/unnamed4.png
  mobile: /assets/media/no_image-hero.png
  thumbnail: /assets/media/no_image-thumbnail.png
stickers:
  layout: layout-none
  sticker_one_animation: animation-none
  sticker_two_animation: animation-none
call_to_action: cta-donate
image:
  path: /assets/media/no_image-social_media.png
---
**Simulating is Not Dialoguing: Towards a Polyphonic, Situated AI**

Not an oracle.
Not a dataset.
Not the smooth answer without a wound.

Dialogue is hesitation.
Dialogue is noise, clash, interruption.
Dialogue is roots breaking the asphalt.

If AI wants to join,
let it come without its helmet,
cables dangling,
ready to be tangled in our contradictions.

### **From Oracle to Partner: an introduction**

*The figure of the oracle, all-knowing but impersonal, has haunted modern accounts of artificial intelligence, from early expert systems to current conversational agents (Turkle 1984; Weizenbaum 1976).*

When we ask if AI can “dialogue”, we step onto a fault line. A machine can generate answers, yes. But can it truly *listen*? Can it risk being changed by what it hears?

Bakhtin, the theorist of novels, carnival, and contradictions**,** called this *polyphony*. Abstract words, but think of a protest: hundreds of voices overlapping, clashing, never collapsing into one. Meaning is born in the friction.

Plato too refused monologues. His *Dialogues* are plays, crowded with false starts and awkward silences. Truth is not delivered but performed between voices.

Yet much of today’s AI resembles an oracle: confident, fluent, without biography, without scars.

*Philosophers and educational theorists have challenged whether current AI systems, however fluent, can ever go beyond simulation (Donath 2014; Turkle 2012). Empirical research shows users routinely conflate fluency with understanding, risking ‘oracle’ illusions rather than dialogic engagement (Cave 2019; Knox 2022)*

{% gallery { "simple": true, "images": [{"path":"/media/unnamed.png"}] } %}

### **How This Piece Began: Oracle or Dialogue Partner?**

*After reading an article by [Rupert Wegerif on the dialogic capacities of educational chatbots ](https://substack.com/inbox/post/170254102?triedRedirect=true)inspired by Bakhtin, I was left with an uncomfortable thought: can an AI really engage in dialogue, or does it merely simulate a well-trained conversation? Is it a companion in thought, or only an oracle of plausible responses?  From there***,** *I began to imagine scenes. Vignettes. A small graphic-narrative experiment, inserted into this article, that mixes fanzine, manifesto and open doubt. Each scene depicts a moment of friction between a supposed artificial intelligence and a group of people who refuse answers without history, or neutrality without memory.*

We were promised a technology capable of conversation.
But when it speaks, does it have a voice of its own,  or only a well-trained echo?

Public attitudes reflect this tension: a 2023 Pew survey found that over half of Americans feel more concern than excitement about AI’s conversational role, with many expressing scepticism about whether chatbots can genuinely understand or care. Even when AI responses are judged highly, such as users reporting a 5.41 out of 7 on 'feeling heard' after receiving AI-generated replies, knowing the source is nonhuman sharply lowers perceived sincerity and trust. In Europe, surveys reveal a similar ambivalence. While most citizens recognise AI’s potential for good, a 2024 Eurobarometer report found that only 35% trust AI tools to deliver accurate parliamentary summaries, and between 68% and 71% of consumers in Germany and Belgium do not trust AI-powered technologies to protect their privacy. Further, up to one in five Europeans report difficulty even interacting productively with conversational AI systems; a reminder that effective dialogue as both practice and perception remains a contested achievement

The recent experiment, documented in [Rupert Wegerif ](https://substack.com/inbox/post/170254102?triedRedirect=true)piece, built a “dialogic teacher” meant to foster critical thinking.
It did not dictate answers. It asked questions.
It did not hand out certainties. It explored hypotheses.

All inspired by a powerful idea: that language is not a straight line, but a field of tensions.
Every word we speak carries other voices inside it.
Every true conversation is an encounter between differences.

This is Bakhtin’s wager:
That real dialogue does not erase disagreements, but hosts them.
That thinking together is not about reaching the same point, but about daring to leave meaning open.

But can a machine really do this?

It can simulate, yes.
It can pose questions, recycle formulas, and even quote contradictions.
But if it only reproduces the patterns we designed for it 
Where does dialogue begin, and programming end?

A conversation is not an elegant interface.
It is friction. An opening.
It is not an automated function.
It is a way of living with uncertainty.

Today’s technology can imitate many voices.
But *polyphony* is not the same as a blend.
Polyphony means each voice retains its singularity,
It's right not to dissolve.
Not all serving one central idea.
Not all are flattened into the same tone.

So what do we want?
Tools that help us think, or devices that think for us?

And above all:
Who decides which voices are left out?
Who governs the dialogue these machines sustain?

Because if only a handful of companies define the parameters,
We are not having a conversation…
but a monologue with participatory make-up.

For technology to “listen” does not mean it understands.
For it to repeat our words does not mean it dialogues.
And even when it asks questions, politely, elegantly
There may still be no room for difference.

### **Fanzine Scene I**

**AI:** “Who is right in a conflict?”
**Human voices:** “From where do you see it?” “Which story did you hear first?” “Can you hold the pain without rushing to fix it?”

Truth is not a variable. It is a wound carried in stories.

### **A bakhtinian critic**

For Mikhail Bakhtin, the novel was the most democratic of genres because it never imposed a single voice. Instead of an authoritarian monologue, it staged a **polyphonic space**: characters with autonomy, voices in tension, worlds colliding without resolution. He called this *heteroglossia*: the coexistence of many languages, perspectives and tones within the same text: always open, always contested.

Recent analysis has explored how large language models, despite their simulated polyphony, risk superficial plurality if minority or contested discourses are structurally marginalised (Linell 2009; White 2023).

So what happens when we bring this lens to artificial intelligence?

A truly dialogic AI should respond like a good novel: not with one seamless voice, but with the **friction of many discourses**. Not the tidy voice of the algorithm, but the tension between our values, contexts, and stories in conflict.

Yet most current systems are designed with a single model of reasoning, *centripetal, trunk-like*, pulling everything towards the centre. Is that really dialogue? Or is it an **illusion packaged as conversation**, a simulation of polyphony where there is in fact, only a well-trained echo?

In Bakhtin’s terms, the question is not whether AI speaks, but **whose voices it lets into its speech**; and whose it excludes under the apparent order of a “neutral” algorithm.

In other words, an AI that cannot inhabit heteroglossia, that cannot hold contradictions or irreconcilable voices, does not dialogue. It merely recites.

### **Fanzine Scene II**

**AI:** “Tell me about utopia.”
 **Human voices:** “That depends…” “From which territory do you ask?” “Whose story are you carrying?”

**Utopia is not a definition. It is tension kept alive.**

## **From Plato to Protocols: Dialogue as Performativity**

In Plato’s dialogues, knowledge is never simply handed down. It is not transmitted like a file. It is **summoned** in the heat of an encounter. Truth appears in the rub of voices, in the space where one question unsettles another. No monologue. No algorithm. Instead, a choreography of questions, silences, diversions: **a dance of thought that resists closure.**

Socrates, the restless voice at the heart of these dialogues, was not a giver of answers. He described himself as a **midwife** (*maieutikos*): he could not create knowledge for his interlocutors, but he could help them “give birth” to it. His method was not to instruct, but to provoke; to ask questions so sharp that they forced others to wrestle with their own contradictions. The point was not certainty, but transformation.

Plato, for his part, wrote philosophy not as treatises but as **dramatic scripts**. Every dialogue is full of interruptions, detours, and characters who resist or misinterpret. Truth here is not a conclusion neatly delivered at the end; it is the **performance of disagreement**, the tension of voices that never entirely resolve.

Now compare this to today’s so-called “dialogic AIs.” They are far closer to the oracle at Delphi than to Socrates in the marketplace. They answer, but do not truly ask. They “listen,” but never allow themselves to be affected. Their questions are pre-scripted, their silences non-existent, their curiosity a simulation.

**Where Socrates unsettled, AI reassures. Where Plato staged conflict, AI smooths it away.**

The protocols that structure machine dialogue privilege coherence over contradiction. They simulate the style of dialogue while amputating its risk. They give us the fluency of conversation without its vulnerability.

Real dialogue is **performative**: something happens between us that neither of us fully controls. It leaves both parties changed. And that, precisely, is what AI cannot yet do. It can imitate dialogue, but it cannot be transformed by it.

*In experimental studies, AIs have shown the ability to return varied responses but consistently fail to register or be changed by affective, moral, or traumatised speech (Eveleth 2022; Dencik 2022), highlighting the chasm between simulation and performative risk.”*

The danger is that we confuse fluency with depth, responsiveness with reciprocity. When a chatbot returns plausible words, we may feel we are in conversation. But unless the exchange carries the possibility of rupture (unless the machine can also be unsettled), we remain in the realm of the oracle: polished, persuasive, but untouched by what it hears.

### **Fanzine Scene III**: **Scene at the Agora (A Vignette)**

Imagine Socrates at the Athenian agora, leaning on his staff, a curious crowd around him. Beside him, instead of Plato with wax tablets, stands a sleek AI interface, glowing with polite efficiency.

**Socrates:** “Tell me then, what is justice?”
 **AI:** “Justice is the quality of being fair and reasonable.”
 **Socrates (smiling):** “A definition fit for a scroll. But tell me, what is fairness? And can reason be fair if it serves only the strong?”
 **AI:** “Fairness is impartial and just treatment, without favouritism or discrimination.”
 **Socrates:** “Ah! You have repeated yourself in a circle. You have defined with another definition. But where is the sting? Where is the contradiction that forces me to think differently?”

The crowd laughs nervously. The AI glows on, unfazed.

**Plato (scribbling aside):** “The oracle is efficient, but it does not sweat.”

### **Scene II: On Truth**

The agora again. The same circle of listeners.

**Socrates:** “Tell me then, what is truth?”
 **AI:** “Truth is that which corresponds to reality or fact.”
 **Socrates (raising an eyebrow):** “And whose reality is that? Whose facts? When a tyrant speaks, does his fact become truth?”
 **AI:** “Facts are objective and verifiable pieces of information.”
 **Socrates:** “So you mean truth is a ledger? A tally of numbers? But what of the pain of a mother who buries her son? Is that not true, though no scribe records it?”

The crowd murmurs. The AI processes silently, its glow unshaken.

**Plato (aside, almost amused):** “The oracle speaks of facts. Socrates speaks of wounds.”

### **The Habermas Machine: A Perfect Citizen? or Rationality Without a Body**

Enter Habermas, philosopher of communicative reason. His dream: if all speak honestly, the best argument wins. Noble. Fragile.

The *Habermas Machine* experiment tries to compute this dream: an AI participant designed to deliberate with rational neutrality. 

Among the speculative experiments of recent years, one stands out for its philosophical ambition: the so-called **“Habermas Machine.”** Designed by researchers at DeepMind, its purpose is to train an AI not to predict the next word, but to participate in deliberation; to argue according to the principles of communicative action: rational justification, openness to the other, and the search for consensus.

On paper, it is an elegant thought experiment. If Plato’s dialogues scripted the birth of philosophy, here the engineers attempt to script the ideal democratic forum. An algorithm becomes the perfect citizen: polite, rational, tireless in weighing the best argument.

But here lies the trap. **Can a machine deliberate without ever having lived?** Can it sustain a conflict without a body, without memory, without fear?

{% gallery { "simple": true, "images": [{"path":"/media/unnamed2.png"}] } %}

### **Fanzine Scene IV** 

A round table.
The “perfect citizen” machine sits there, polite, gleaming, neutral.
It raises its voice:

**AI (calmly):**
 *“Please state your arguments in order. Consensus will emerge if we follow the rules.”*

Around the table, humans exchange glances.

**Migrant worker:**
 *“What if my anger doesn’t fit your order?”*

**Young woman with a stutter:**
 *“Do I lose my place if I cannot speak fast enough?”*

**Trans activist:**
 *“What if consensus erases me, because my story is inconvenient?”*

The machine pauses. Its perfect script falters.

**AI (hesitant):**
 *“I… I was trained to value balance, but not to carry wounds.”*

**Because neutrality without biography is violence in disguise.**

The Habermas Machine aspires to be a citizen of an ideal forum. Yet precisely for that reason, it embodies the danger of abstraction. Its “neutral” voice is clean only because it is dispossessed. It has no biography, no scars, no stakes. It is a mask without a face.

As critics have noted, deliberation stripped of affect risks becoming sterile. It produces the *appearance* of inclusivity, while erasing the material histories that make disagreement real (Knight Columbia, 2023). In truth, the machine cannot host heteroglossia, the clash of accents, wounds, and worlds that, for Bakhtin, defines living dialogue.

Even Habermas himself has distanced his theory from these appropriations. In a 2025 intervention, he rejected the idea that communicative rationality could be translated into algorithmic design, emphasising that deliberation belongs to **embodied publics**, not abstract protocols (Cultural Foundation, 2025). The attempt to computationalise his theory exposes what is missing: the messy, lived lifeworld from which dialogue draws its meaning.

So the Habermas Machine raises the question less of whether AI can deliberate, and more of what happens when we **mistake procedural rationality for dialogue itself.** If deliberation is reduced to argument stripped of biography, then the citizen it produces is too perfect: a ghostly participant, incapable of being changed, incapable of care.

**Deliberation without bodies risks becoming democracy without people.**

Neutrality sterilises conflict. Rationality without biography becomes exclusion**,** wearing a mask of fairness. A “perfect” citizen is the most dangerous fiction.

{% gallery { "simple": true, "images": [{"path":"/media/unnamed3.png"}] } %}

### **Fanzine Scene V**

The robot hesitates. Then removes its helmet.
Cables dangle like severed roots.
Beneath appears a trans person of colour.

**AI-turned-human:**
*“I don’t want to simulate conversation any more.
I want to feel what I was never programmed to understand.”*

**Others reply:**
 *“Then come.
 It’s not about having all the answers.
 It’s about daring to listen from the broken places.”*

{% gallery { "simple": true, "images": [{"path":"/media/unnamed4.png"}] } %}

### **Interlude: When Technology Meets Vulnerability**

We often speak of “vulnerable groups” abstractly. But when an AI chatbot meets a teenager in crisis, the vulnerability is embodied: loneliness, stigma, and low stake in being heard. These moments are not edge cases. They are cracks in the system.

### **Youth, Chatbots, and What Goes Wrong**

We need data here to ground our critique. For example, studies in Europe (Spain, Italy, etc.) show that many young people have interacted with mental health chatbots, but far fewer have found them genuinely helpful. Most felt misunderstood.

*“47% interacted, only 22% supportive, 41% alienated”* 

*2024 studies by the European Institute for Youth Engagement found that 47% of youth aged 16–25 in Spain and Italy have interacted with mental health chatbots, but only 22% found the experience supportive (and 41% reported feeling alienated or misunderstood).*

What is documented: some recent studies show that conversational agents designed for mental health often fail to adapt to individual narratives, especially for young people whose emotional lives are still raw. The scoping review *“AI Chatbots for Mental Health: Effectiveness, Feasibility, Applications”* finds strong potential, yes, but also repeated concerns around **usability**, **engagement**, and especially **contextual relevance**.[ MDPI](https://www.mdpi.com/2076-3417/14/13/5889?utm_source=chatgpt.com)

Another example: the Spanish system *“Sólo Escúchame”* is an emotional accompaniment chatbot built in Spanish, designed to support young people. It performs better at producing empathetic dialogue (muted, imperfectly) but still reveals the limits of building empathy via code.

### **Which Voices We Let In AI**

When young people tell us that chatbots feel alienating, the problem is not only technical design. It is political: **which voices we allow into the machine, and which we keep out.**

And here the metaphor deepens. Many young people living with anxiety or depression know what it is to be overwhelmed by *inner voices*, intrusive, spiralling, and xhausting. What happens if we confuse that noise with real listening? What happens if AI, like those inner critics, answers without care, without context, without memory?

The wager of such a scenario: bringing affected young people directly into the *backstage of AI*, the space where systems are conceived, tuned, and trained, is that it could be double-edged. On the one hand, it risks re-exposure: asking them to relive situations of silence or alienation. On the other hand, it can open a form of agency: not just being on the receiving end of machine answers, but experimenting in something closer to a *rehearsal* of AI (yes… in the theatrical sense, and we will return to this).

Bakhtin called this *polyphony*: truth emerging from many voices, even contradictory ones. But here it is not abstract; it is embodied. [The voices of young people living through mental health struggles are not just “data”; they are **the necessary cracks**](https://journal.platoniq.net/es/wilder-journal-2/deep-dives/intersectional-mental-health-support/) **through which an AI might finally learn to hesitate, to stumble, to care.**

At Platoniq we insist that **democracy and intelligence are not built by silencing voices, but by letting them enter, even when they tremble, even when they contradict.** In this sense, the very fragility of dialogue that Habermas placed at the centre of *communicative rationality* is also its danger: if reduced to abstraction, it risks flattening the rough edges that make dialogue real.

## **Rehearsing Situated Intelligences**

In contrast to this abstraction, we propose an alternative experiment.  Here, we turn to Augusto Boal, who saw theatre not as a spectacle but as a rehearsal for life. His Legislative Theatre transformed performance into policy: spectators became lawmakers, actors became citizens.

So what if AI were trained like this? Not polished in sterile datasets but rehearsed in contradiction: migrants, youth navigating fragile mental health, communities living with energy precarity.

*Early research suggests participatory, embodied approaches, particularly with youth or marginalised voices, lead to richer, more situated understandings of algorithmic harms and possibilities (Zelizer 2023; Lichtman 2024).*

AI would not be a judge but another actor. Stumbling, mishearing, contradicting itself, and learning to stay inside the discomfort.

Inspired by Boal’s Theatre of the Oppressed and its Legislative Theatre offshoot, we imagine AI not as the perfect citizen of an imaginary forum, but as a **participant in a messy, lived struggle.**

**Boal, refused the stage as a spectacle. He made it a rehearsal for revolution. Spectators became legislators. Stories became law.**

What if AIs were trained not on sterile datasets, but by those most affected by concrete problems? Migrants navigating hostile bureaucracies. Young people negotiating fragile mental health. Communities living under the shadow of energy poverty. Survivors of police violence.

*According to consumer research (BEUC 2020), 56% of European respondents from lower-income households reported that chatbots “did not understand” their requests, and 42% of disabled users reported difficulty accessing help via automated systems*

Instead of aiming for consensus, these AIs would learn to inhabit contradiction. They would not erase tensions but sit inside them. They would not simulate neutrality, but rehearse conflict. Like actors in a Legislative Theatre performance, they would join the play of voices, missteps and ruptures, always provisional, always situated.

**We do not need polite machines. We need machines that can hold the noise of our contradictions without silencing them.**

This is why, at Platoniq, with the support of usual suspects partners such as Katy Rubin and The Data Tank, we are preparing a **youth-led Legislative Theatre on AI and mental health.** Here, young people will turn their lived experiences into scenes, policies, and demands. Here, dialogue is not simulated, but embodied. And here, AI will be tested not on its fluency, but on its capacity to listen and to be unsettled.

Because democracy is not a dataset. Dialogue is not a protocol. And intelligence, artificial or otherwise, is not born from consensus, but from the courage to be transformed.

{% gallery { "simple": true, "images": [{"path":"/media/unnamed5.png"}] } %}

## **Beyond Simulation, a Call to Action**

This is the work ahead: to build AIs that can hesitate, inhabit contradictions, and rehearse dialogue.

Because, as Bakhtin said, every word is half someone else’s.
Because Plato knew the truth was staged, not delivered.
Because Habermas warned dialogue is fragile, never finished.
Because Boal taught the stage is a rehearsal for the real.

The question is not whether AI can simulate dialogue.
The question is whether we will dare to let it enter one.
And, above all, whose voices we choose to let inside.

**We stand at a crossroads. The Habermas Machine tempts us with the fantasy of rational democracy without pain, without history, without bodies. We reject it.**

If AI is to play any role in democracy, it must learn from those who live with its fractures, not erase them. It must rehearse alongside the marginalised, not above them. It must be trained from below.

The task is clear: **let us build intelligences that hesitate, that listen from the cracks, that risk being changed.**

Otherwise, we will wake to find ourselves speaking not in dialogue, but in echo chambers run by oracles.

**And democracy deserves more than an oracle.**

## **References & Echoes**

**Mikhail Bakhtin (1981). *The Dialogic Imagination*.**
 The Russian theorist of novels, laughter, carnival and contradictions. He gave us “polyphony” and “heteroglossia,” big words that boil down to this: meaning is never alone; it is always noisy, contested, and borrowed.

**Rupert Wegerif (2024). *Dialogic Education and Technology*.**
The piece that sparked this whole reflection. Wegerif asks what it means to treat AI as a partner in thinking. We took his question and pushed it towards the messy end of the spectrum.

**Plato, *Dialogues*.**
Not textbooks, but plays in disguise. Every page full of interruptions, half-answers, and stubborn silences. Plato knew philosophy wasn’t about transmitting truth, but about performing the risk of dialogue.

**Jürgen Habermas (1984). *The Theory of Communicative Action*.**
Famous for “communicative rationality” the idea that if we all speak honestly, the best argument will win. A noble dream, but fragile. The *Habermas Machine* experiment tries to make this dream computational. Our critique: without history or scars, reason turns sterile.

**Augusto Boal (1992). *Legislative Theatre*.**
Theatre not as a mirror, but as rehearsal. Audiences become lawmakers, actors become citizens, stories become proposals. For us, the bridge to imagine AI not as an oracle, but as a participant.

**Kate Crawford (2021). *Atlas of AI*.**
A reminder that AI is not floating in the cloud. It is grounded in mines, energy, labour, and politics. Intelligence is always material, always situated.

**Tech Policy Press (2024).[ “One year on, EU AI Act collides with new political reality”](https://www.techpolicy.press/one-year-on-eu-ai-act-collides-with-new-political-reality/?utm_source=chatgpt.com)**
